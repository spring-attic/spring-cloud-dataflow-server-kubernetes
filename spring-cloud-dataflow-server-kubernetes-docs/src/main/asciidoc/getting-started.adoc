= Getting Started

== Deploying Streams on Kubernetes

In this getting started guide, the Data Flow Server is run as a standalone application outside the Kubernetes cluster.  A future version will allow the Data Flow Server itself to run on Kubernetes.

. Deploy a Kubernetes cluster.
+ 
The http://kubernetes.io/docs/getting-started-guides/[Kubernetes getting started guide] lets you choose among many deployment options so you can pick one that you are most comfortable using. We have successfully used the Vagrant option from a downloaded Kubernetes release.
+
Of note, the https://github.com/olmoser/docker-compose-kubernetes[docker-compose-kubernetes] is not among those options, but it was also used by the developers of this project to run a local Kubernetes cluster using Docker Compose.  
+
The rest of this getting started guide assumes that you have a working Kubernetes cluster and a `kubectl` command line.
+
. Create a redis service on the Kubernetes cluster.
+
The redis service will be used for messaging between modules in the stream.  There are sample replication controller and service YAML files in the `spring-cloud-dataflow-admin-kubernetes` repository that you can use as a starting point as they have the required metadata set for service discovery by the modules.
+
If you are deploying a Kubernetes cluster outside your local machine, setting the service to be a LoadBalancer type will give you an external IP address to connect to the redis instance.  
+ 
```
$ git clone https://github.com/spring-cloud/spring-cloud-dataflow-admin-kubernetes
$ cd spring-cloud-dataflow-admin-kubernetes
$ kubectl create -f src/etc/kubernetes/redis-controller.yml
$ kubectl create -f src/etc/kubernetes/redis-service.yml
```
You can use the command `kubectl get pods` to verify that the controller is running.  Note that it can take a minute or so until there is an external IP address for the redis server.  Use the command `kubectl get services` to check on the state of the service and look for when there is a value under the EXTERNAL_IP column. The command `kubectl delete service redis` to clean up afterwards.
+
. Determine the location of your Kubernetes Master URL, for example:
+
```
$ kubectl cluster-info

Kubernetes master is running at https://10.245.1.2

   ...other output omitted...
```
. Export environment variables to connect to Kubernetes.
+
The Data Flow Server uses the https://github.com/fabric8io/kubernetes-client[fabric8 Java client library] to connect to the Kubernetes cluster.  It can be configured using system properites, environment variables, and the Kube config file.  In testing using the https://cloud.google.com/container-engine/docs/[Google Container Engine], only setting the environment variables `KUBERNETES_MASTER` and `KUBERNETES_NAMESPACE` were required.  Other configuration values were read from the Kube config file.
+
```
$ export KUBERNETES_MASTER=https://10.245.1.2/
$ export KUBERNETES_NAMESPACE=default
```
+
This approach supports using one Data Flow Server instance per Kubernetes namespace.
+
. Run a local Redis server.
+
```
$ cd <redis-install-dir> 
$ ./src/redis-server
```
+
This is used by the locally running Data Flow Server to store the state of available module versions for stream definitions.
+
NOTE: You could potentially use the Redis service deployed in Kubernetes as long as it is deployed with service type `LoadBalancer` and the node that it is running on is reachable from your system. You should be able to provide the IP address of the node and the nodePort used using arguments for the Redis connection like `--spring.redis.host=10.245.1.3 --spring.redis.port=31986` with the server startup command below.
+
NOTE: If you are switching between milestone and snapshot versions of the Data Flow Server, flush the redis keys that contain the module's version information.  Calling `flushdb` is indiscriminate but a handy way to start from a clean redis state.
+
. Download and run the Spring Cloud Data Flow Server for Kubernetes.
+
```
$ wget http://repo.spring.io/milestone/org/springframework/cloud/spring-cloud-dataflow-admin-kubernetes/1.0.0.M1/spring-cloud-dataflow-server-kubernetes-1.0.0.M1.jar

$ java -jar spring-cloud-dataflow-server-kubernetes-1.0.0.M1.jar
```
+
Ensure that the Data Flow Server is running in the same terminal session that has the Kubernetes environment variables set.
+
. Download and run the Spring Cloud Data Flow shell.
+
```
$ wget http://repo.spring.io/snapshot/org/springframework/cloud/spring-cloud-dataflow-shell/1.0.0.BUILD-SNAPSHOT/spring-cloud-dataflow-shell-1.0.0.BUILD-SNAPSHOT.jar

$ java -jar spring-cloud-dataflow-shell-1.0.0.BUILD-SNAPSHOT.jar
```
+
. Deploy a simple stream in the shell
+
```
dataflow:>stream create --name ticktock --definition "time | log" --deploy
```
+
You can use the command `kubectl get pods` to check on the state of the pods corresponding to this stream.  
+
```
$ kubectl get pods
NAME                  READY     STATUS    RESTARTS   AGE
redis-d207a           1/1       Running   0          50m
ticktock-log-qnk72    1/1       Running   0          2m
ticktock-time-r65cn   1/1       Running   0          2m
```
+
Look at the logs for the pod deployed for the log sink.
+
```
$ kubectl logs -f ticktock-log-qnk72
...
2015-12-28 18:50:02.897  INFO 1 --- [           main] o.s.c.s.module.log.LogSinkApplication    : Started LogSinkApplication in 10.973 seconds (JVM running for 50.055)
2015-12-28 18:50:08.561  INFO 1 --- [hannel-adapter1] log.sink                                 : 2015-12-28 18:50:08
2015-12-28 18:50:09.556  INFO 1 --- [hannel-adapter1] log.sink                                 : 2015-12-28 18:50:09
2015-12-28 18:50:10.557  INFO 1 --- [hannel-adapter1] log.sink                                 : 2015-12-28 18:50:10
2015-12-28 18:50:11.558  INFO 1 --- [hannel-adapter1] log.sink                                 : 2015-12-28 18:50:11
```
+
A useful command to help in troubleshooting issues, such as a container that has a fatal error starting up, add the options `--previous` to view last terminated container log. You can also get more detailed information about the pods by using the `kubctl describe` like:
+
```
kubectl describe pods/ticktock-log-qnk72
```
+
. Destroy the stream
+
```
dataflow:>stream destroy --name ticktock
```
NOTE: If you stop and restart the Data Flow Server server after streams are deployed, you will not be able to destroy them.  This is a bug that is being addressed in a future release.

