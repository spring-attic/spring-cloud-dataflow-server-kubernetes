[[kubernetes-getting-started]]
= Getting Started

http://cloud.spring.io/spring-cloud-dataflow/[Spring Cloud Data Flow] is a toolkit for building data integration and real-time data processing pipelines.

Pipelines consist of Spring Boot apps, built using the Spring Cloud Stream or Spring Cloud Task microservice frameworks. This makes Spring Cloud Data Flow suitable for a range of data processing use cases, from import/export to event streaming and predictive analytics.

This project provides support for using Spring Cloud Data Flow with Kubernetes as the runtime for these pipelines with apps packaged as Docker images.

== Deploying the Spring Cloud Data Flow server on Kubernetes

In this section we will deploy the Spring Cloud Data Flow Server to a Kubernetes cluster.
Operationalizing Spring Cloud Data Flow depends on few services and its availability.
For example, we need an RDBMS service for the app registry, stream, and task repositories. 
For streaming pipelines, we also need a transport option such as Apache Kafka or Rabbit MQ. 
In addition to this, we need a Redis service if the analytics features are in use.

[IMPORTANT]
====
This guide describes setting up an environment for testing Spring Cloud Data Flow on Google Container Engine and is not meant to be a definitive guide for setting up a production environment. Feel free to adjust the suggestions to fit your test set-up. Please remember that a production environment requires much more consideration for persistent storage of message queues, high availability, security etc.
====

[NOTE]
====
Currently, only apps registered with a `--uri` property pointing to a Docker resource are supported by the Data Flow Server for Kubernetes. 

Note that we do support Maven resources for the `--metadata-uri` property.

E.g. the below app registration is valid:

[source,console,subs=attributes]
----
dataflow:>app register --type source --name time --uri docker://springcloudstream/time-source-rabbit:{scst-starters-core-version} --metadata-uri maven://org.springframework.cloud.stream.app:time-source-rabbit:jar:metadata:{scst-starters-core-version}
----

but any app registered with a Maven, HTTP or File resource for the executable jar (using a `--uri` property prefixed with `maven://`, `http://` or `file://`) is *_not supported_*.
====

. Create a Kubernetes cluster.
+ 
The Kubernetes https://kubernetes.io/docs/setup/pick-right-solution/[Picking the Right Solution] guide lets you choose among many options so you can pick one that you are most comfortable using.
+
All our testing is done using the https://cloud.google.com/container-engine/[Google Container Engine] that is part of the Google Cloud Platform. That is a also the target platform for this getting started chapter. We have also successfully deployed using https://kubernetes.io/docs/getting-started-guides/minikube/[Minikube] and we will note where you need to adjust for deploying on Minikube.
+
NOTE: When starting Minikube you should allocate some extra resources since we will be deploying several services. We have used `minikube start --cpus=4 --memory=4096` to start.
+
The rest of this getting started guide assumes that you have a working Kubernetes cluster and a `kubectl` command line utility. See the docs for installation instructions: http://kubernetes.io/docs/user-guide/prereqs/[Installing and Setting up kubectl].
+
. Get the Kubernetes configuration files.
+
NOTE: We are developing a https://helm.sh/[Helm] chart to simplify all this and we plan on making it available as part of https://kubeapps.com/[KubeApps].
+
There are sample deployment and service YAML files in the https://github.com/{github-repo}[https://github.com/{github-repo}] repository that you can use as a starting point. They have the required metadata set for service discovery by the different apps and services deployed. To check out the code enter the following commands:
+ 
[source,console,subs=attributes]
----
$ git clone https://github.com/{github-repo}
$ cd spring-cloud-dataflow-server-kubernetes
$ git checkout {github-tag}
----
+
. Create a Rabbit MQ service on the Kubernetes cluster.
+
The Rabbit MQ service will be used for messaging between modules in the stream.  You could also use Kafka, but, in order to simplify, we only show the Rabbit MQ configurations in this guide.
+
Run the following commands to start the Rabbit MQ service:
+
```
$ kubectl create -f src/kubernetes/rabbitmq/
```
+
You can use the command `kubectl get all -l app=rabbitmq` to verify that the deployment, pod and service resources are running. Use the command `kubectl delete all -l app=rabbitmq` to clean up afterwards.
+
. Create a MySQL service on the Kubernetes cluster.
+
We are using MySQL for this guide, but you could use Postgres or H2 database instead. We include JDBC drivers for all three of these databases, you would just have to adjust the database URL and driver class name settings.
+
IMPORTANT: You can modify the password in the `src/kubernetes/mysql/mysql-deployment.yaml` files if you prefer to be more secure. If you do modify the password you will also have to provide it base64 encoded in the `src/kubernetes/mysql/mysql-secrets.yaml` file. 
+
Run the following commands to start the MySQL service:
+
```
$ kubectl create -f src/kubernetes/mysql/
```
You can use the command `kubectl get all -l app=mysql` to verify that the deployment, pod and service resources are running. Use the command `kubectl delete all,pvc,secrets -l app=mysql` to clean up afterwards.
+
. Create a Redis service on the Kubernetes cluster.
+
The Redis service will be used for the analytics functionality. Run the following commands to start the Redis service:
+
```
$ kubectl create -f src/kubernetes/redis/
```
+
NOTE: If you don't need the analytics functionality you can turn this feature off by changing `SPRING_CLOUD_DATAFLOW_FEATURES_ANALYTICS_ENABLED` to false in the `src/kubernetes/server/server-deployment.yml` file. If you don't install the Redis service then you should also remove the Redis configuration settings in `src/kubernetes/server/server-config-kafka.yml` mentioned below.
+
You can use the command `kubectl get all -l app=redis` to verify that the deployment, pod and service resources are running. Use the command `kubectl delete all -l app=redis` to clean up afterwards.
+
. Deploy the Metrics Collector on the Kubernetes cluster.
+
The Metrics Collector will provide message rates for all deployed stream apps. These message rates will be visible in the Dashboard UI. Run the following commands to start the Metrics Collector:
+ 
```
$ kubectl create -f src/kubernetes/metrics/metrics-deployment-rabbit.yaml
$ kubectl create -f src/kubernetes/metrics/metrics-svc.yaml
```
+
You can use the command `kubectl get all -l app=metrics` to verify that the deployment, pod and service resources are running. Use the command `kubectl delete all -l app=metrics` to clean up afterwards.
+
. Update configuration files with values needed to connect to the required services.
+
IMPORTANT: You should specify the version of the Spring Cloud Data Flow server that you want to deploy.
+
The deployment is defined in the `src/kubernetes/server/server-deployment.yaml` file. To control what version of the Spring Cloud Data Flow server that gets deployed you should modify the tag used for the Docker image in the container spec:
+
[source,yaml]
----
    spec:
      containers:
      - name: scdf-server
        image: springcloud/spring-cloud-dataflow-server-kubernetes:latest    # <1>
        imagePullPolicy: Always
----
+
<1> change `latest` to the version you would like. This document is based on the `{project-version}` version so the recommended image tag to use for this is `{docker-tag}`.
+
The Data Flow Server uses the https://github.com/fabric8io/kubernetes-client[Fabric8 Java client library] to connect to the Kubernetes cluster.  We are using environment variables to set the values needed when deploying the Data Flow server to Kubernetes. We are also using the https://github.com/fabric8io/spring-cloud-kubernetes[Fabric8 Spring Cloud integration with Kubernetes library] to access Kubernetes http://kubernetes.io/docs/user-guide/configmap/[ConfigMap] and http://kubernetes.io/docs/user-guide/secrets/[Secrets] settings.
The ConfigMap settings are specified in the `src/kubernetes/server/server-config-rabbit.yaml` file and the secrets are in the `src/kubernetes/mysql/mysql-secrets.yaml` file. If you modified the password for MySQL you should have changed it in the `src/kubernetes/mysql/mysql-secrets.yaml` file. Any secrets have to be provided base64 encoded.
+
NOTE: We are now configuring the Data Flow server with file based security and the default user is 'user' with a password of 'password'. Feel free to change this in the `src/kubernetes/server/server-config-rabbit.yaml` file.
+
We haven't tuned the memory use of the OOTB apps yet, so to be on the safe side we are increasing the memory for the pods by providing the following environment variable in the `src/kubernetes/server/server-deployment.yaml` file:
+
```
        - name: SPRING_CLOUD_DEPLOYER_KUBERNETES_MEMORY
          value: 640Mi
```
+
. Deploy the Spring Cloud Data Flow Server for Kubernetes using the Docker image and the configuration settings.
+
```
$ kubectl create -f src/kubernetes/server/server-config-rabbit.yaml
$ kubectl create -f src/kubernetes/server/server-svc.yaml
$ kubectl create -f src/kubernetes/server/server-deployment.yaml
```
+
You can use the command `kubectl get all -l app=scdf-server` to verify that the deployment, pod and service resources are running. Use the command `kubectl delete all,cm -l app=scdf-server` to clean up afterwards.
+
Use the `kubectl get svc scdf-server` command to locate the EXTERNAL_IP address assigned to `scdf-server`, we will use that later to connect from the shell.
+
```
$ kubectl get svc
NAME         CLUSTER-IP       EXTERNAL-IP       PORT(S)    AGE
scdf-server  10.103.246.82    130.211.203.246   80/TCP     4m
```
So the URL you need to use is in this case http://130.211.203.246
+
If you are using Minikube then you don't have an external load balancer and the EXTERNAL-IP will show as `<pending>`. You need to use the NodePort assigned for the `scdf-server` service. Use this command to look up the URL to use:
+
```
$ minikube service --url scdf-server
http://192.168.99.100:31991
```

== Creating and Running Streams on Kubernetes

. Download and run the Spring Cloud Data Flow shell.
+
[subs=attributes]
```
wget http://repo.spring.io/{dataflow-version-type-lowercase}/org/springframework/cloud/spring-cloud-dataflow-shell/{dataflow-project-version}/spring-cloud-dataflow-shell-{dataflow-project-version}.jar

$ java -jar spring-cloud-dataflow-shell-{dataflow-project-version}.jar
```
+
That should give you the following startup message from the shell:
+
[subs=attributes]
```
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/

{dataflow-project-version}

Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
server-unknown:>
```
+
Configure the Data Flow server URI with the following command (use the URL determined above in the previous step) using the default user and password settings:
+
```
server-unknown:>dataflow config server --username user --password password --uri http://130.211.203.246/
Successfully targeted http://130.211.203.246/
dataflow:>
```
+
. Register the Docker with Rabbit binder versions of the `time` and `log` apps using the shell.
+
[subs=attributes]
```
dataflow:>app register --type source --name time --uri docker://springcloudstream/time-source-rabbit:{scst-starters-core-version} --metadata-uri maven://org.springframework.cloud.stream.app:time-source-rabbit:jar:metadata:{scst-starters-core-version}
dataflow:>app register --type sink --name log --uri docker://springcloudstream/log-sink-rabbit:{scst-starters-core-version} --metadata-uri maven://org.springframework.cloud.stream.app:log-sink-rabbit:jar:metadata:{scst-starters-core-version}
```
+
. Alternatively, if you would like to register all out-of-the-box stream applications built with the Rabbit binder in bulk, 
you can with the following command. For more details, review how to link:http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/html/spring-cloud-dataflow-register-apps.html[register applications].
+
```
dataflow:>app import --uri http://bit.ly/stream-applications-rabbit-docker
```
+
. Deploy a simple stream in the shell
+
```
dataflow:>stream create --name ticktock --definition "time | log" --deploy
```
+
You can use the command `kubectl get pods` to check on the state of the pods corresponding to this stream. We can run this from the shell by running it as an OS command by adding a "!" before the command.
+
```
dataflow:>! kubectl get pods -l role=spring-app
command is:kubectl get pods -l role=spring-app
NAME                  READY     STATUS    RESTARTS   AGE
ticktock-log-0-qnk72  1/1       Running   0          2m
ticktock-time-r65cn   1/1       Running   0          2m
```
+
Look at the logs for the pod deployed for the log sink.
+
```
dataflow:>! kubectl logs ticktock-log-0-qnk72
command is:kubectl logs ticktock-log-0-qnk72
...
2017-07-20 04:34:37.369  INFO 1 --- [time.ticktock-1] log-sink                                 : 07/20/17 04:34:37
2017-07-20 04:34:38.371  INFO 1 --- [time.ticktock-1] log-sink                                 : 07/20/17 04:34:38
2017-07-20 04:34:39.373  INFO 1 --- [time.ticktock-1] log-sink                                 : 07/20/17 04:34:39
2017-07-20 04:34:40.380  INFO 1 --- [time.ticktock-1] log-sink                                 : 07/20/17 04:34:40
2017-07-20 04:34:41.381  INFO 1 --- [time.ticktock-1] log-sink                                 : 07/20/17 04:34:41
```
+
. Destroy the stream
+
```
dataflow:>stream destroy --name ticktock
```
+
A useful command to help in troubleshooting issues, such as a container that has a fatal error starting up, add the options `--previous` to view last terminated container log. You can also get more detailed information about the pods by using the `kubctl describe` like:
+
```
kubectl describe pods/ticktock-log-qnk72
```
+
NOTE: If you need to specify any of the app specific configuration properties then you might use "long-form" of them including the app specific prefix like `--jdbc.tableName=TEST_DATA`. This form is *required* if you didn't register the `--metadata-uri` for the Docker based starter apps. In this case you will also not see the configuration properties listed when using the `app info` command or in the Dashboard GUI.

=== Accessing app from outside the cluster

If you need to be able to connect to from outside of the Kubernetes cluster to an app that you deploy, like the `http-source`, then you need to use either an external load balancer for the incoming connections or you need to use a NodePort configuration that will expose a proxy port on each Kubetnetes Node. If your cluster doesn't support external load balancers, like the Minikube, then you must use the NodePort approach. You can use deployment properties for configuring the access. Use  `deployer.http.kubernetes.createLoadBalancer=true` for the app to specify that you want to have a LoadBalancer with an external IP address created for your app's service. For the NodePort configuration use `deployer.http.kubernetes.createNodePort=<port>` where `<port>` should be a number between 30000 and 32767.

. Register the `http-source`, you can use the following command:
+
[subs=attributes]
```
dataflow:>app register --type source --name http --uri docker:springcloudstream/http-source-rabbit:{scst-starters-core-version} --metadata-uri maven://org.springframework.cloud.stream.app:http-source-rabbit:jar:metadata:{scst-starters-core-version}
```
+
. Create the `http | log` stream without deploying it using the following command:
+
```
dataflow:>stream create --name test --definition "http | log"
```
+
. If your cluster supports an External LoadBalancer for the `http-source`, then you can use the following command to deploy the stream:
+
```
dataflow:>stream deploy test --properties "deployer.http.kubernetes.createLoadBalancer=true"
```
Wait for the pods to be started showing 1/1 in the READY column by using this command:
+
```
dataflow:>! kubectl get pods -l role=spring-app
command is:kubectl get pods -l role=spring-app
NAME               READY     STATUS    RESTARTS   AGE
test-http-2bqx7    1/1       Running   0          3m
test-log-0-tg1m4   1/1       Running   0          3m
```
Now, look up the external IP address for the `http` app (it can sometimes take a minute or two for the external IP to get assigned):
+
```
dataflow:>! kubectl get service test-http
command is:kubectl get service test-http
NAME         CLUSTER-IP       EXTERNAL-IP      PORT(S)    AGE
test-http    10.103.251.157   130.211.200.96   8080/TCP   58s
```
. If you are using Minikube, or any cluster that doesn't support an External LoadBalancer, then you should deploy the stream with a NodePort in the range of 30000-32767. Use the following command to deploy it:
+
```
dataflow:>stream deploy test --properties "deployer.http.kubernetes.createNodePort=32123"
```
+
Wait for the pods to be started showing 1/1 in the READY column by using this command:
+
```
dataflow:>! kubectl get pods -l role=spring-app
command is:kubectl get pods -l role=spring-app
NAME               READY     STATUS    RESTARTS   AGE
test-http-9obkq    1/1       Running   0          3m
test-log-0-ysiz3   1/1       Running   0          3m
```
Now look up the URL to use with the following command:
+
```
dataflow:>! minikube service --url test-http
command is:minikube service --url test-http
http://192.168.99.100:32123
```
+
. Post some data to the `test-http` app either using the EXTERNAL-IP address from above with port 8080 or the URL provided by the minikube command:
+
```
dataflow:>http post --target http://130.211.200.96:8080 --data "Hello"
```
+
. Finally, look at the logs for the `test-log` pod:
+
```
dataflow:>! kubectl get pods-l role=spring-app
command is:kubectl get pods-l role=spring-app
NAME              READY     STATUS             RESTARTS   AGE
test-http-9obkq   1/1       Running            0          2m
test-log-0-ysiz3  1/1       Running            0          2m
dataflow:>! kubectl logs test-log-0-ysiz3
command is:kubectl logs test-log-0-ysiz3
...
2016-04-27 16:54:29.789  INFO 1 --- [           main] o.s.c.s.b.k.KafkaMessageChannelBinder$3  : started inbound.test.http.test
2016-04-27 16:54:29.799  INFO 1 --- [           main] o.s.c.support.DefaultLifecycleProcessor  : Starting beans in phase 0
2016-04-27 16:54:29.799  INFO 1 --- [           main] o.s.c.support.DefaultLifecycleProcessor  : Starting beans in phase 2147482647
2016-04-27 16:54:29.895  INFO 1 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)
2016-04-27 16:54:29.896  INFO 1 --- [  kafka-binder-] log.sink                                 : Hello
```
+
. Destroy the stream
+
```
dataflow:>stream destroy --name test
```

== Creating and Launching Tasks on Kubernetes

. Create a task and launch it
+
Let's register the `timestamp` task app and create a simple task definition and launch it.
+
[subs=attributes]
```
dataflow:>app register --type task --name timestamp --uri docker:springcloudtask/timestamp-task:{sct-starters-core-version} --metadata-uri maven://org.springframework.cloud.task.app:timestamp-task:jar:metadata:{sct-starters-core-version}
dataflow:>task create task1 --definition "timestamp"
dataflow:>task launch task1
```
We can now list the tasks and executions using these commands:
+
[options=nowrap]
```
dataflow:>task list
╔═════════╤═══════════════╤═══════════╗
║Task Name│Task Definition│Task Status║
╠═════════╪═══════════════╪═══════════╣
║task1    │timestamp      │running    ║
╚═════════╧═══════════════╧═══════════╝

dataflow:>task execution list 
╔═════════╤══╤════════════════════════════╤════════════════════════════╤═════════╗
║Task Name│ID│         Start Time         │          End Time          │Exit Code║
╠═════════╪══╪════════════════════════════╪════════════════════════════╪═════════╣
║task1    │1 │Fri May 05 18:12:05 EDT 2017│Fri May 05 18:12:05 EDT 2017│0        ║
╚═════════╧══╧════════════════════════════╧════════════════════════════╧═════════╝
```
+
. Destroy the task
+
```
dataflow:>task destroy --name task1
```

== Application Configuration

This section covers how you can customize the deployment of your applications. You can use a number of deployer properties to influence settings for the applications that are deployed.

See https://github.com/spring-cloud/spring-cloud-deployer-kubernetes/blob/master/src/main/java/org/springframework/cloud/deployer/spi/kubernetes/KubernetesDeployerProperties.java[KubernetesDeployerProperties] for more of the supported options.

If you would like to override the default values for all apps that you deploy then you should modify the <<index.html#_spring_cloud_deployer_for_kubernetes_properties, Spring Cloud Deployer for Kubernetes Properties>> for the server.

=== Memory and CPU Settings

The apps are deployed by default with the following "Limits" and "Requests" settings:

```
    Limits:
      cpu:	500m
      memory:	512Mi
    Requests:
      cpu:	500m
      memory:	512Mi
```

You might find that the 512Mi memory limit is too low and to increase it you can provide a common `spring.cloud.deployer.memory` deployer property like this (replace <app> with the name of the app you would like to set this for):

```
deployer.<app>.memory=640m
```

This property affects bot the Requests and Limits memory value set for the container.

If you would like to set the Requests and Limits values separately you would have to use the deployer properties that are specific to the Kubernetes deployer. To set the Limits to 1000m for cpu, 1024Mi for memory and Requests to 800m for cpu, 640Mi for memory you can use the following properties:

```
deployer.<app>.kubernetes.limits.cpu=1000m
deployer.<app>.kubernetes.limits.memory=1024Mi
deployer.<app>.kubernetes.requests.cpu=800m
deployer.<app>.kubernetes.requests.memory=640Mi
```

That should result in the following container settings being used:

```
    Limits:
      cpu:	1
      memory:	1Gi
    Requests:
      cpu:	800m
      memory:	640Mi
```

NOTE: When using the common memory property you should use and `m` suffix for the value while when using the Kubernetes specific properties you should use the Kubernetes `Mi` style suffix.

The settings we have used so far only affect the settings for the container, they do not affect the memory setting for the JVM process in the container. If you would like to set JVM memory settings you can provide an environment variable for this, see the next section for details.

=== Environment Variables

To influence the environment settings for a given app, you can take advantage of the `spring.cloud.deployer.kubernetes.environmentVariables` deployer property. 
For example, a common requirement in production settings is to influence the JVM memory arguments.
This can be achieved by using the `JAVA_TOOL_OPTIONS` environment variable:

```
deployer.<app>.kubernetes.environmentVariables=JAVA_TOOL_OPTIONS=-Xmx1024m
```

This overrides the JVM memory setting for the desired <app> (just replace <app> with the name of your app). 

=== Liveness and Readiness Probes

The _liveness_ and _readiness_ probes are using the _paths_ `\health` and `\info` respectively. They use a _delay_ of 10 for both and a _period_ of 60 and 10 respectively. You can chage these defaults when you deploy by using deployer properties.

Here is an example changing the _liveness_ probe (just replace <app> with the name of your app):

```
deployer.<app>.kubernetes.livenessProbePath=/info
deployer.<app>.kubernetes.livenessProbeDelay=120
deployer.<app>.kubernetes.livenessProbePeriod=20
```

Similarly, swap _liveness_ for _readiness_ to override the default readiness settings.

