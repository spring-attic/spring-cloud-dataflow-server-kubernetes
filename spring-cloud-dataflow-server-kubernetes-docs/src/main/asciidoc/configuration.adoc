[[configuration]]
= Server Configuration

[partintro]
--
In this section you will learn how to configure Spring Cloud Data Flow server's features such as the relational database to use and security.
--

[[enable-disable-specific-features]]
== Feature Toggles

Data Flow server offers specific set of features that can be enabled/disabled when launching. These features include all the lifecycle operations, REST endpoints (server, client implementations including Shell and the UI) for:

. Streams
. Tasks
. Analytics

You can enable or disable these features by setting the following boolean environment variables when launching the Data Flow server:

* `SPRING_CLOUD_DATAFLOW_FEATURES_STREAMS_ENABLED`
* `SPRING_CLOUD_DATAFLOW_FEATURES_TASKS_ENABLED`
* `SPRING_CLOUD_DATAFLOW_FEATURES_ANALYTICS_ENABLED`

By default, all the features are enabled.

NOTE: Since analytics feature is enabled by default, the Data Flow server is expected to have a valid Redis store available as analytic repository as we provide a default implementation of analytics based on Redis. This also means that the Data Flow server's `health` depends on the redis store availability as well. If you do not want to enable HTTP endpoints to read analytics data written to Redis, then disable the analytics feature using the property mentioned above.

The REST endpoint `/features` provides information on the features enabled/disabled.

[[configuration-general]]
== General Configuration

The Spring Cloud Data Flow server for Kubernetes uses the Fabric8 https://github.com/fabric8io/spring-cloud-kubernetes[`spring-cloud-kubernetes`] module to process both ConfigMap and Secrets settings. You just need to enable the ConfigMap support by passing in an environment variable of `SPRING_CLOUD_KUBERNETES_CONFIG_NAME` and setting that to the name of the ConfigMap. Same is true for the Secrets where the environment variable is `SPRING_CLOUD_KUBERNETES_SECRETS_NAME`. To use the Secrets you also need to set `SPRING_CLOUD_KUBERNETES_SECRETS_ENABLE_API` to true.

Here is an example of a snippet from a deployment that sets these environment variables.

[source,yaml]
----
        env:
        - name: SPRING_CLOUD_KUBERNETES_SECRETS_ENABLE_API
          value: 'true'
        - name: SPRING_CLOUD_KUBERNETES_SECRETS_NAME
          value: mysql
        - name: SPRING_CLOUD_KUBERNETES_CONFIG_NAME
          value: scdf-server
----

=== Using ConfigMap and Secrets

Configuration properties can be passed to the Data Flow Server using Kubernetes https://kubernetes.io/docs/tasks/configure-pod-container/configmap/[ConfigMap] and https://kubernetes.io/docs/concepts/configuration/secret/[Secrets]. 

An example configuration could look like the following where we configure RabbitMQ, MySQL and Redis as well as basic security settings for the server:

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: scdf-server
  labels:
    app: scdf-server
data:
  application.yaml: |-
    security:
      basic:
        enabled: true
        realm: Spring Cloud Data Flow
    spring:
      cloud:
        dataflow:
          security:
            authentication:
              file:
                enabled: true
                users:
                  admin: admin, ROLE_MANAGE, ROLE_VIEW
                  user: password, ROLE_VIEW, ROLE_CREATE
        deployer:
          kubernetes:
            environmentVariables: 'SPRING_RABBITMQ_HOST=${RABBITMQ_SERVICE_HOST},SPRING_RABBITMQ_PORT=${RABBITMQ_SERVICE_PORT},SPRING_REDIS_HOST=${REDIS_SERVICE_HOST},SPRING_REDIS_PORT=${REDIS_SERVICE_PORT}'
      datasource:
        url: jdbc:mysql://${MYSQL_SERVICE_HOST}:${MYSQL_SERVICE_PORT}/mysql
        username: root
        password: ${mysql-root-password}
        driverClassName: org.mariadb.jdbc.Driver
        testOnBorrow: true
        validationQuery: "SELECT 1"
      redis:
        host: ${REDIS_SERVICE_HOST}
        port: ${REDIS_SERVICE_PORT}
----

We assume here that RabbitMQ is deployed using `rabbitmq` as the service name. For MySQL we assume the service name is `mysql` and for Redis we assume it is `redis`. Kubernetes will publish these services' host and port values as environment variables that we can use when configuring the apps we deploy.

We prefer to provide the MySQL connection password in a Secrets file:

[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: mysql
  labels:
    app: mysql
data:
  mysql-root-password: eW91cnBhc3N3b3Jk
----

The password is provided as a base64 encoded value.

[[configuration-rdbms]]
== Database Configuration

Spring Cloud Data Flow provides schemas for H2, HSQLDB, MySQL, Oracle, PostgreSQL, DB2 and SQL Server that will be automatically created when the server starts.

The JDBC drivers for *MySQL* (via MariaDB driver), *HSQLDB*, *PostgreSQL* along with embedded *H2* are available out of the box.
If you are using any other database, then the corresponding JDBC driver jar needs to be on the classpath of the server.

For instance,
If you are using *MySQL* in addition to password in the Secrets file provide the following properties in the ConfigMap:

[source,yaml]
----
data:
  application.yaml: |-
    spring:
      datasource:
        url: jdbc:mysql://${MYSQL_SERVICE_HOST}:${MYSQL_SERVICE_PORT}/mysql
        username: root
        password: ${mysql-root-password}
        driverClassName: org.mariadb.jdbc.Driver
        url: jdbc:mysql://${MYSQL_SERVICE_HOST}:${MYSQL_SERVICE_PORT}/test
        driverClassName: org.mariadb.jdbc.Driver
----

For *PostgreSQL*:

[source,yaml]
----
data:
  application.yaml: |-
    spring:
      datasource:
        url: jdbc:postgresql://${PGSQL_SERVICE_HOST}:${PGSQL_SERVICE_PORT}/database
        username: root
        password: ${postgres-password}
        driverClassName: org.postgresql.Driver
----

For *HSQLDB*:

[source,yaml]
----
data:
  application.yaml: |-
    spring:
      datasource:
        url: jdbc:hsqldb:hsql://${HSQLDB_SERVICE_HOST}:${HSQLDB_SERVICE_PORT}/database
        username: sa
        driverClassName: org.hsqldb.jdbc.JDBCDriver
----

NOTE: There is a schema update to the Spring Cloud Data Flow datastore when upgrading from version `1.0.x` to `1.1.x` and from `1.1.x` to `1.2.x`.
Migration scripts for specific database types can be found in the https://github.com/spring-cloud/spring-cloud-task/tree/master/spring-cloud-task-core/src/main/resources/org/springframework/cloud/task/migration[spring-cloud-task] repo.


[[configuration-security]]
== Security

We are now securing the server application in the sample configurations file used in the <<kubernetes-getting-started,Getting Started section>>.

This section covers the basic configuration settings we provide in the provided sample configuration, please refer to the  link:http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/htmlsingle/#configuration-security[core security documentation] for more detailed coverage of the security configuration options for the Spring Cloud Data Flow server and shell.

When using RabbitMQ as the transport, the security settings are located in the `src/kubernetes/server/server-config-rabbit.yaml` file and when using Kafka the settings are located in the `src/kubernetes/server/server-config-kafka.yaml` file:
[source,yaml]
----
    security:
      basic:
        enabled: true                                         # <1>
        realm: Spring Cloud Data Flow                         # <2>
    spring:
      cloud:
        dataflow:
          security:
            authentication:
              file:
                enabled: true
                users:
                  admin: admin, ROLE_MANAGE, ROLE_VIEW        # <3>
                  user: password, ROLE_VIEW, ROLE_CREATE      # <4>
----

<1> Enable security
<2> Optionally set the realm, defaults to "Spring"
<3> Create an 'admin' user with password set to 'admin' that can view apps, streams and tasks and that can also view management endpoints
<4> Create a 'user' user with password set to 'password' than can register apps and create streams and tasks and also view them

Feel free to change user names and passwords to suite, and also maybe move the definition of user passwords to a Kubernetes Secret.

[[configuration-monitoring-management]]
== Monitoring and Management

We recommend using the `kubectl` command for troubleshooting streams and tasks. 

You can list all artifacts and resources used by using the following command:

[source,shell]
----
kubectl get all,cm,secrets,pvc
----

You can list all resources used by a specific app or service by using a label to select resources. The following command list all resources used by the `mysql` service:

[source,shell]
----
kubectl get all -l app=mysql
----

You can get the logs for a specific pod by issuing:

[source,shell]
----
kubectl logs pod <pod-name>
----

If the pod is continuously getting restarted you can add `-p` as an option to see the previous log like:

[source,shell]
----
kubectl logs -p <pod-name>
----

You can also tail or follow a log by adding an `-f` option:

[source,shell]
----
kubectl logs -f <pod-name>
----

A useful command to help in troubleshooting issues, such as a container that has a fatal error starting up, is to use the describe command like:

[source,shell]
----
kubectl describe pod ticktock-log-0-qnk72
----

=== Inspecting Server Logs

You can access the server logs by using the following command (just supply the name of pod for the server):

[source,shell]
----
kubectl get pod -l app=scdf=server
kubectl logs <scdf-server-pod-name>
----

=== Streams

The stream apps are deployed with the stream name followed by the name of the app and for processors and sinks there is also an instance index appended. 

To see all the pods that are deployed by the Spring Cloud Data Flow server you can specify the label `role=spring-app`:

[source,shell]
----
kubectl get pod -l role=spring-app
----

To see details for a specific app deployment you can use (just supply the name of pod for the app):

[source,shell]
----
kubectl describe pod <app-pod-name>
----

For the application logs use:

[source,shell]
----
kubectl logs <app-pod-name>
----

If you would like to tail a log you can use:

[source,shell]
----
kubectl logs -f <app-pod-name>
----

=== Tasks

Tasks are launched as bare pods without a replication controller. The pods remain after the tasks complete and this gives you an opportunity to review the logs. 

To see all pods for a specific task use this command while providing the task name:

[source,shell]
----
kubectl get pod -l task-name=<task-name>
----

To review the task logs use:

[source,shell]
----
kubectl logs <task-pod-name>
----

You have two options to delete completed pods. You can delete them manually once they are no longer needed.

To delete the task pod use:

[source,shell]
----
kubectl delete pod <task-pod-name>
----

You can also use the Data Flow shell command `task execution cleanup` command to remove the completed pod for a task execution.

First we need to determine the `ID` for the task execution:

[source,shell]
----
dataflow:>task execution list 
╔═════════╤══╤════════════════════════════╤════════════════════════════╤═════════╗
║Task Name│ID│         Start Time         │          End Time          │Exit Code║
╠═════════╪══╪════════════════════════════╪════════════════════════════╪═════════╣
║task1    │1 │Fri May 05 18:12:05 EDT 2017│Fri May 05 18:12:05 EDT 2017│0        ║
╚═════════╧══╧════════════════════════════╧════════════════════════════╧═════════╝
----

Next we issue the command to cleanup the execution artifacts (the completed pod):

[source,shell]
----
dataflow:>task execution cleanup --id 1
Request to clean up resources for task execution 1 has been submitted
----

== Debug Support

Debugging the Spring Cloud Data Flow Kubernetes Server and included components such as the https://github.com/spring-cloud/spring-cloud-deployer-kubernetes[Spring Cloud Kubernetes Deployer] is supported through the https://docs.oracle.com/javase/8/docs/technotes/guides/jpda/jdwp-spec.html[Java Debug Wire Protocol (JDWP)]. This section will outline an approach to manually enable debugging and another that uses configuration files provided with Spring Cloud Data Flow Server Kubernetes to "patch" a running deployment.

NOTE: JDWP itself does not use any authentication. This section assumes debugging is being done on a local development environment such as Minikube and guidance on securing the debug port is not provided.

=== Enabling Debugging Manually

To manually enable JDWP, first edit `src/kubernetes/server/server-deployment.yaml` and add an additional `containerPort` entry under `spec.template.spec.containers.ports` with a value of `5005`. Additionally add the environment variable https://docs.oracle.com/javase/8/docs/platform/jvmti/jvmti.html#tooloptions[`JAVA_TOOL_OPTIONS`] under `spec.template.spec.containers.env` as shown below:

```
spec:
  ...
  template:
    ...
    spec:
      containers:
      - name: scdf-server
        ...
        ports:
        ...
		- containerPort: 5005
        env:
        - name: JAVA_TOOL_OPTIONS
          value: '-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005'
```

NOTE: Port 5005 is used in this example, but it can be any number that does not conflict with another port. The chosen port number must also be the same for the added `containerPort` value along with the `address` parameter of the `JAVA_TOOL_OPTIONS` `-agentlib` flag as shown above.

The Spring Cloud Data Flow Kubernetes Server can now be started as normal. Once the server is up, changes from above can be verified on the `scdf-server` deployment:

[source,shell]
----
$ kubectl describe deployment/scdf-server
...
...
Pod Template:
  ...
  Containers:
   scdf-server:
    ...
    Ports:       80/TCP, 5005/TCP
    ...
    Environment:
      JAVA_TOOL_OPTIONS:  -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005
	  ...
----

With the server started and JDWP enabled, access to the port needs to be configured. In this example we will use the https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/[`port-forward`] subcommand of `kubectl`. Exposing a local port to our debug target using `port-forward` can be done as follows:

[source,shell]
----
$ kubectl get pod -l app=scdf-server
NAME                           READY     STATUS    RESTARTS   AGE
scdf-server-5b7cfd86f7-d8mj4   1/1       Running   0          10m
$ kubectl port-forward scdf-server-5b7cfd86f7-d8mj4 5005:5005
Forwarding from 127.0.0.1:5005 -> 5005
Forwarding from [::1]:5005 -> 5005
----

A debugger may now be attached by pointing it to `127.0.0.1` as the host and `5005` as the port. The `port-forward` subcommand will remain running until killed with for example, `CTRL+c`.

Debugging support can be removed by reverting the changes to `src/kubernetes/server/server-deployment.yaml`. The reverted changes will be picked up on the next deployment of the Spring Cloud Data Flow Kubernetes Server. Manually adding debug support to the configuration comes in useful when debugging should be enabled by default each time the server is deployed.

=== Enabling Debugging with Patching

Rather than manually changing the `server-deployment.yaml`, created Kubernetes Objects can be "patched" in place. For convenience patch files are included that provide the same configuration as the manual approach. To enable debugging by patching, enter the following:

[source,shell]
----
$ kubectl patch deployment scdf-server -p "$(cat src/kubernetes/server/server-deployment-debug.yaml)"
----

Running this command will automatically add the `containerPort` attribute and the `JAVA_TOOL_OPTIONS` environment variable. Changes to the `scdf-server` deployment can be verified by running the following command:

[source,shell]
----
$ kubectl describe deployment/scdf-server
...
...
Pod Template:
  ...
  Containers:
   scdf-server:
    ...
    Ports:       5005/TCP, 80/TCP
    ...
    Environment:
      JAVA_TOOL_OPTIONS:  -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005
	  ...
----

To enable access to the debug port, rather than using the `port-forward` subcommand of `kubectl`, another option would be to patch the `scdf-server` Kubernetes Service Object. Patching this object can be done as follows:

First ensure the `scdf-server` Kubernetes Service Object has the proper configuration.

[source,shell]
----
$ kubectl describe service/scdf-server
----

If the output contains the text `<unset>`, for example:

```
Port:                     <unset>  80/TCP
TargetPort:               80/TCP
NodePort:                 <unset>  30784/TCP
```

Patch the service to add a name for this port:

[source,shell]
----
$ kubectl patch service scdf-server -p "$(cat src/kubernetes/server/server-svc.yaml)"
----

NOTE: A port name should only be missing if the target cluster had been created prior to debug functionality being added. Since multiple ports are being added to the `scdf-server` Kubernetes Service Object, each needs to have their own name.

Now add the debug port:

[source,shell]
----
$ kubectl patch service scdf-server -p "$(cat src/kubernetes/server/server-svc-debug.yaml)"
----

To verify the mapping:

[source,shell]
----
$ kubectl describe service scdf-server
Name:                     scdf-server
...
...
Port:                     scdf-server-jdwp  5005/TCP
TargetPort:               5005/TCP
NodePort:                 scdf-server-jdwp  31339/TCP
...
...
Port:                     scdf-server  80/TCP
TargetPort:               80/TCP
NodePort:                 scdf-server  30883/TCP
...
...
----

In the output, its shown that that container port 5005 has been mapped to the NodePort of 31339. To get the IP address of the Minikube node enter the following:

[source,shell]
----
$ minikube ip
192.168.99.100
----

With this information a debug connection can now be created using a host of 192.168.99.100 and a port of 31339.

To disable JDWP, the following commands can be used:

[source,shell]
----
$ kubectl rollout undo deployment/scdf-server
$ kubectl patch service scdf-server --type json -p='[{"op": "remove", "path": "/spec/ports/0"}]'
----

The Kubernetes Deployment Object is rolled back to its state prior before being patched. The Kubernetes Service Object is then patched with a `remove` operation to remove port 5005 from the `containerPorts` list.

NOTE: `kubectl rollout undo` will force the pod to restart. Patching the Kubernetes Service Object will not recreate the service and the port mapping to the `scdf-server` deployment will remain the same.

See https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-a-deployment[Rolling Back a Deployment] for more information on deployment rollbacks including managing history and https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/[Update API Objects in Place Using kubectl Patch] for more information on patching.
